<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>ID²</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="./resources/teasor.png"/>
  	<meta property="og:title" content="VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models." />
  	<meta property="og:description" content="VidProM is the first dataset featuring 1.67 million unique text-to-video prompts and 6.69 million videos generated from 4 different state-of-the-art diffusion models. It inspires many exciting new research areas, such as Text-to-Video Prompt Engineering, Efficient Video Generation, Fake Video Detection, and Video Copy Detection for Diffusion Models." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on prompts, and there is no publicly available dataset that features a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 Million unique text-to-Video Prompts from real users. Additionally, this dataset includes 6.69 million videos generated by four state-of-the-art diffusion models, alongside some related data. We initially discuss the curation of this large-scale dataset, a process that is both time-consuming and costly. Subsequently, we underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Our extensive and diverse dataset also opens up many exciting new research areas. For instance, we suggest exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models to develop better, more efficient, and safer models." />
    <meta property="twitter:title"         content="VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models." />
    <meta property="twitter:description"   content="VidProM is the first dataset featuring 1.67 million unique text-to-video prompts and 6.69 million videos generated from 4 different state-of-the-art diffusion models. It inspires many exciting new research areas, such as Text-to-Video Prompt Engineering, Efficient Video Generation, Fake Video Detection, and Video Copy Detection for Diffusion Models." />
    <meta property="twitter:image"         content="./resources/teasor.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
       Origin Identification for Text-Guided Image-to-Image Diffusion Models
    </div>

    <div class="venue">
        ICML 2025
    </div>

    <br><br>

    <div class="author">
        <a href="https://wangwenhao0716.github.io/">Wenhao Wang</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://yifansun-reid.github.io/">Yifan Sun</a><sup>2</sup>
    </div>
    <div class="author">
        <a href="https://z-x-yang.github.io/">Zongxin Yang</a><sup>3</sup>
    </div>
    <div class="author">
        <a href="https://scholar.google.com.hk/citations?user=jDsfBUwAAAAJ">Zhentao Tan</a><sup>4</sup>
    </div>
    <div class="author">
        <a href="https://scholar.google.com/citations?user=Udl0uiMAAAAJ">Zhengdong Hu</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ">Yi Yang</a><sup>2</sup>
    </div>
    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>University of Technology Sydney</div>
    <div class="affiliation"><sup>2&nbsp;</sup>Zhejiang University</div>
    <div class="affiliation"><sup>3&nbsp;</sup>Harvard University</div>
    <div class="affiliation"><sup>4&nbsp;</sup>Peking University</div>

    <br><br>

    <div class="links"><a href="https://arxiv.org/abs/2501.02376">[Paper]</a></div>
    <div class="links"><a href="https://huggingface.co/datasets/WenhaoWang/OriPID">[Data]</a></div>
    <div class="links"><a href="https://github.com/WangWenhao0716/ID2">[Github]</a></div>
    <div class="links"><a href="https://openreview.net/forum?id=46n3izUNiv">[OpenReview]</a></div>

    <br><br>

    <img style="width: 80%;" src="./resources/teasor.png" alt="Teaser figure."/>
    <br>
    <p style="width: 80%;">
        Modern AI tools can now rewrite any picture just by following a short text instruction—turning a daytime street into a rainy night scene, or adding new objects that never existed. While fun and useful, this power makes it easy to spread fake images, dodge copyright rules, and hide the true source of a picture. We tackle this problem by asking a simple but crucial question: given an edited image, can we reliably find the original photo it came from?
    </p>

    <br><br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
        Text-guided image-to-image diffusion models excel in translating images based on textual prompts, allowing for precise and creative visual modifications. However, such a powerful technique can be misused for <em>spreading misinformation</em>, <em>infringing on copyrights</em>, and <em>evading content tracing</em>. This motivates us to introduce the task of origin <strong>ID</strong>entification for text-guided <strong>I</strong>mage-to-image <strong>D</strong>iffusion models (<strong>ID<sup>2</sup></strong>), aiming to retrieve the original image of a given translated query. A straightforward solution to ID<sup>2</sup> involves training a specialized deep embedding model to extract and compare features from both query and reference images. However, due to <em>visual discrepancy</em> across generations produced by different diffusion models, this similarity-based approach fails when training on images from one model and testing on those from another, limiting its effectiveness in real-world applications. To solve this challenge of the proposed ID<sup>2</sup> task, we contribute the first dataset and a theoretically guaranteed method, both emphasizing generalizability. The curated dataset, <strong>OriPID</strong>, contains abundant <strong>Ori</strong>gins and guided <strong>P</strong>rompts, which can be used to train and test potential <strong>ID</strong>entification models across various diffusion models. In the method section, we first prove the <em>existence</em> of a linear transformation that minimizes the distance between the pre-trained Variational Autoencoder embeddings of generated samples and their origins. Subsequently, it is demonstrated that such a simple linear transformation can be <em>generalized</em> across different diffusion models. Experimental results show that the proposed method achieves satisfying generalization performance, significantly surpassing similarity-based methods (+31.6&nbsp;% mAP), even those with generalization designs.
    </p>

    <br><br>
    <hr>

    <h1>Datapoint</h1>
    <img style="width: 80%;" src="./resources/datapoint.jpg"
         alt="A data point in the proposed VidProM"/>
    <br>
    
    <br><br>
    <hr>

    <h1>Basic information of VidProM and DiffusionDB</h1>
    <img style="width: 80%;" src="./resources/compare_table.jpg"
         alt="Results figure"/>

    <br><br>
    <hr>

    <h1>Differences between prompts in VidProM and DiffusionDB</h1>
    <img style="width: 80%;" src="./resources/compare_visual.png"
         alt="Results figure"/>

    <br><br>
    <hr>

    <h1>WizMap visualization of prompts in VidProM and DiffusionDB</h1>
    <img style="width: 80%;" src="./resources/WizMap_V_D.jpg"
         alt="Results figure"/>
    <div class="links"><a href="https://poloclub.github.io/wizmap/?dataURL=https://huggingface.co/datasets/WenhaoWang/VidProM/resolve/main/data_vidprom_diffusiondb.ndjson&gridURL=https://huggingface.co/datasets/WenhaoWang/VidProM/resolve/main/grid_vidprom_diffusiondb.json%20">[Wizmap]</a></div>
    <br><br>
    <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org/abs/2403.06098">
            <img class="layered-paper-big" width="100%" src="./resources/vidprom.png" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models</h3>
        <p>Wenhao Wang and Yi Yang</p>
        <p>NeurIPS, 2024.</p>
        <pre><code>@article{wang2024vidprom,
  title={VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models},
  author={Wang, Wenhao and Yang, Yi},
  journal={Thirty-eighth Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=pYNl76onJL}
}</code></pre>
    </div>

    <br><br>
    <hr>

   <h1>Contact</h1>
    <p style="width: 80%;">
        If you have any questions, feel free to contact <a href="https://wangwenhao0716.github.io/">Wenhao Wang</a> (wangwenhao0716@gmail.com).
    </p>

      <br><br>
    <hr>

  <h1>Works using VidProM</h1>
    <p style="width: 80%;">
      We are actively collecting your awesome works using our VidProM. Please let us know if you finish one.
    </p>
      
    <p style="width: 80%;">
    1. Ji, Lichuan, et al. "Distinguish Any Fake Videos: Unleashing the Power of Large-scale Data and Motion Features." Arxiv 2024.
    </p>
    <p style="width: 80%;">
    2. Xuan, He, et al. "VIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation." EMNLP 2024.
    </p>
    <p style="width: 80%;">
    3. Liao, Mingxiang, et al. "Evaluation of Text-to-Video Generation Models: A Dynamics Perspective." NeurIPS 2024.
    </p>
    <p style="width: 80%;">
    4. Miao, Yibo, et al. "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models." NeurIPS 2024.
    </p>
    <p style="width: 80%;">
    5. Wu, Xun, et al. "Boosting Text-to-Video Generative Model with MLLMs Feedback." NeurIPS 2024.
    </p>
    <p style="width: 80%;">
    6. Dai, Josef, et al. "SAFESORA: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset." NeurIPS 2024.
    </p>
    <p style="width: 80%;">
    7. Liu, Joseph, et al. "SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers." Arxiv 2024.
    </p>
    <p style="width: 80%;">
    8. Wang, Zeqing, et al. "Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body." Arxiv 2024.
    </p>
    <p style="width: 80%;">
    9. Jang, MinHyuk, et al. "LVMark: Robust Watermark for latent video diffusion models." Arxiv 2024.
    </p>
    <p style="width: 80%;">
    10. Liu, Runtao, et al. "VideoDPO: Omni-Preference Alignment for Video Diffusion Generation." Arxiv 2024.
    </p>
    <p style="width: 80%;">
    11. Ji, Jiaming, et al. "Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback." Arxiv 2024.
    </p>
    <p style="width: 80%;">
    12. Xu, Jiazheng, et al. "VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation" Arxiv 2024.
    </p>
    <p style="width: 80%;">
    13. Wang, Zihan, et al. "What You See Is What Matters: A Novel Visual and Physics-Based Metric for Evaluating Video Generation Quality" Arxiv 2024.
    </p>
    <p style="width: 80%;">
    14. Sun, Desen, et al. "FlexCache: Flexible Approximate Cache System for Video Diffusion" Arxiv 2025.
    </p>
    <p style="width: 80%;">
    15. Li, Lijun, et al. "T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation" Arxiv 2025.
    </p>
    <p style="width: 80%;">
    16. Ni, Zhenliang, et al. "GenVidBench: A Challenging Benchmark for Detecting AI-Generated Video" Arxiv 2025.
    </p>
    <p style="width: 80%;">
    17. Wang, Wenhao, et al. "VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation" Arxiv 2025.
    </p>
    <p style="width: 80%;">
    18. Cheng, Jiale, et al. "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization" Arxiv 2025.
    </p>
    <p style="width: 80%;">
    19. Xue, Zeyue, et al. "DanceGRPO: Unleashing GRPO on Visual Generation" Arxiv 2025.
    </p>
    <p style="width: 80%;">
    20. Liu, Jiayang, et al. "Jailbreaking the Text-to-Video Generative Models" Arxiv 2025.
    </p>
    <p style="width: 80%;">
    21. Li, Lijun, et al. "Benchmarking Ethics in Text-to-Image Models: A Holistic Dataset and Evaluator for Fairness, Toxicity, and Privacy" OpenReview 2025.
    </p>
      

      

     
 

   <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a> and <a href="https://github.com/elliottwu/webpage-template">Shangzhe Wu</a>.
        The code can be found <a href="https://github.com/VidProM/vidprom.github.io">here</a>.
    </p>

    <br><br>
</div>

</body>

</html>
